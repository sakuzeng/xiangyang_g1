#!/usr/bin/env python3
"""
screen_target_locator.py
========================

åŸºäºYOLOæ£€æµ‹çš„å±å¹•ç›®æ ‡å®šä½ç³»ç»Ÿ

åŠŸèƒ½:
1. è°ƒç”¨å¤–éƒ¨YOLOæœåŠ¡è¯†åˆ«å±å¹•åŒºåŸŸ
2. æ ¹æ®æŒ‡å®šç¼–å·è·å–ç›®æ ‡åŒºåŸŸä¸­å¿ƒç‚¹
3. å°†åƒç´ åæ ‡è½¬æ¢ä¸ºæœºå™¨äººTorsoåæ ‡ç³»
4. æ”¯æŒå®æ—¶é¢„è§ˆå’Œäº¤äº’å¼æ“ä½œ

ä¾èµ–:
- RealSenseCamera (æ‘„åƒå¤´æ¥å£)
- requests (HTTPè¯·æ±‚)
- scipy (åæ ‡è½¬æ¢)
"""

import sys
import os
import cv2
import numpy as np
import requests
import pyrealsense2 as rs
from scipy.spatial.transform import Rotation as R
from typing import Optional, Tuple, Dict, Any
from pathlib import Path

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'loco', 'unitree_sdk_python'))

from unitree_sdk2py.camera.realsense_camera_client import RealSenseCamera


# ==========================================
# åæ ‡è½¬æ¢å™¨ (ä» camera_to_torso.py ç§»æ¤)
# ==========================================
class CoordTransformer:
    """ç›¸æœºåæ ‡ç³» -> Torsoåæ ‡ç³»è½¬æ¢å™¨"""
    
    def __init__(self):
        # URDF åŸå§‹å¹³ç§»å‚æ•°
        self.urdf_trans = np.array([0.0576235, 0.01753, 0.42987])
        
        # æ ¡å‡†å¥½çš„ Pitch
        self.pitch_offset = 0.23
        self.base_pitch = 0.8307767239493009
        
        self._recalc_matrices()
    
    def _recalc_matrices(self):
        """é‡æ–°è®¡ç®—æ—‹è½¬çŸ©é˜µ"""
        final_pitch = self.base_pitch + self.pitch_offset
        
        # Optical -> Link
        self.mat_opt_to_link = np.array([
            [0, 0, 1],
            [-1, 0, 0],
            [0, -1, 0]
        ])
        
        # Link -> Torso
        self.urdf_rpy = [0, final_pitch, 0]
        r_obj = R.from_euler('xyz', self.urdf_rpy, degrees=False)
        try:
            self.mat_link_to_torso = r_obj.as_matrix()
        except:
            self.mat_link_to_torso = r_obj.as_dcm()
    
    def process(self, point_cam_optical: np.ndarray) -> np.ndarray:
        """
        å°†ç›¸æœºå…‰å­¦åæ ‡ç³»çš„ç‚¹è½¬æ¢åˆ°Torsoåæ ‡ç³»
        
        Args:
            point_cam_optical: ç›¸æœºåæ ‡ç³»ä¸‹çš„3Dç‚¹ [x, y, z]
        
        Returns:
            np.ndarray: Torsoåæ ‡ç³»ä¸‹çš„3Dç‚¹ [x, y, z]
        """
        P_opt = np.array(point_cam_optical)
        P_link = self.mat_opt_to_link @ P_opt
        P_torso = self.mat_link_to_torso @ P_link + self.urdf_trans
        return P_torso


# ==========================================
# YOLO æœåŠ¡å®¢æˆ·ç«¯
# ==========================================
class YOLOClient:
    """YOLOå±å¹•æ£€æµ‹æœåŠ¡å®¢æˆ·ç«¯"""
    
    def __init__(self, server_url: str = "http://192.168.77.103:28000"):
        self.server_url = server_url.rstrip('/')
        self.endpoint = f"{self.server_url}/yolo"
    
    def detect_screen_target(self, image: np.ndarray, target_index: int) -> Optional[Dict[str, Any]]:
        """
        è°ƒç”¨YOLOæœåŠ¡æ£€æµ‹å±å¹•å¹¶è¿”å›ç›®æ ‡åŒºåŸŸä¿¡æ¯
        
        Args:
            image: BGRå›¾åƒ
            target_index: ç›®æ ‡åŒºåŸŸç¼–å· (0-35)
        
        Returns:
            DictåŒ…å«:
                - found: bool
                - screen_corners: [[x,y], ...]  # å±å¹•å››è§’ç‚¹
                - target_region: {
                    "center": (x, y),
                    "corners": [[x,y], ...]
                  }
        """
        try:
            # ç¼–ç å›¾åƒ
            _, img_encoded = cv2.imencode('.jpg', image, [cv2.IMWRITE_JPEG_QUALITY, 95])
            
            # å‘é€è¯·æ±‚
            files = {'file': ('image.jpg', img_encoded.tobytes(), 'image/jpeg')}
            data = {'target_index': target_index}
            
            response = requests.post(
                self.endpoint,
                files=files,
                data=data,
                timeout=5.0
            )
            
            if response.status_code == 200:
                result = response.json()
                return result
            else:
                print(f"âŒ YOLOæœåŠ¡é”™è¯¯: {response.status_code}")
                return None
                
        except requests.exceptions.Timeout:
            print("âŒ YOLOæœåŠ¡è¶…æ—¶")
            return None
        except Exception as e:
            print(f"âŒ YOLOæœåŠ¡è°ƒç”¨å¤±è´¥: {e}")
            return None


# ==========================================
# æ·±åº¦è¾…åŠ©å·¥å…· (ä» camera_to_torso.py ç§»æ¤)
# ==========================================
class DepthHelper:
    """æ·±åº¦å›¾è¾…åŠ©å·¥å…·"""
    
    @staticmethod
    def get_precise_depth(depth_image: np.ndarray, x: int, y: int, 
                         max_search_radius: int = 20) -> Tuple[float, Tuple[int, int]]:
        """
        æ™ºèƒ½æ·±åº¦æœç´¢
        
        Returns:
            (depth_value, (offset_x, offset_y))
        """
        height, width = depth_image.shape
        
        # è¾¹ç•Œæ£€æŸ¥
        if not (0 <= x < width and 0 <= y < height):
            return 0, (0, 0)
        
        # ç­–ç•¥1: ç›´æ¥è¯»å–ä¸­å¿ƒåƒç´ 
        center_depth = depth_image[y, x]
        if center_depth > 0:
            return center_depth, (0, 0)
        
        # ç­–ç•¥2: åŒå¿ƒåœ†æ‰©æ•£æœç´¢
        for radius in range(1, max_search_radius + 1):
            candidates = []
            num_samples = max(8, radius * 2)
            
            for i in range(num_samples):
                angle = 2 * np.pi * i / num_samples
                dx = int(radius * np.cos(angle))
                dy = int(radius * np.sin(angle))
                
                nx, ny = x + dx, y + dy
                
                if 0 <= nx < width and 0 <= ny < height:
                    depth_val = depth_image[ny, nx]
                    if depth_val > 0:
                        candidates.append((depth_val, dx, dy, radius))
            
            if candidates:
                depths = [c[0] for c in candidates]
                median_depth = np.median(depths)
                best_candidate = min(candidates, key=lambda c: abs(c[0] - median_depth))
                depth_val, dx, dy, r = best_candidate
                return depth_val, (dx, dy)
        
        # ç­–ç•¥3: åŒºåŸŸä¸­å€¼å¡«è¡¥
        x_min = max(0, x - max_search_radius)
        x_max = min(width, x + max_search_radius + 1)
        y_min = max(0, y - max_search_radius)
        y_max = min(height, y + max_search_radius + 1)
        
        roi = depth_image[y_min:y_max, x_min:x_max]
        valid_pixels = roi[roi > 0]
        
        if len(valid_pixels) > 10:
            median_val = np.median(valid_pixels)
            return median_val, (0, 0)
        
        return 0, (0, 0)


# ==========================================
# ä¸»åº”ç”¨ç±»
# ==========================================
class ScreenTargetLocator:
    """å±å¹•ç›®æ ‡å®šä½å™¨"""
    
    def __init__(self, yolo_server_url: str = "http://192.168.77.103:28000"):
        # ç»„ä»¶åˆå§‹åŒ–
        self.camera = RealSenseCamera(width=848, height=480, fps=30)
        self.yolo_client = YOLOClient(yolo_server_url)
        self.coord_transformer = CoordTransformer()
        self.depth_helper = DepthHelper()
        
        # çŠ¶æ€å˜é‡
        self.current_target_index = 0  # é»˜è®¤ç›®æ ‡ç¼–å·
        self.last_detection_result = None
        self.last_torso_coords = None
        
        print(f"âœ… å±å¹•ç›®æ ‡å®šä½å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   YOLOæœåŠ¡: {yolo_server_url}")
        print(f"   æ‘„åƒå¤´åˆ†è¾¨ç‡: 848x480")
    
    def detect_and_locate(self, color_image: np.ndarray, depth_raw: np.ndarray, 
                         target_index: int) -> Optional[Dict[str, Any]]:
        """
        æ£€æµ‹å±å¹•å¹¶å®šä½ç›®æ ‡åŒºåŸŸçš„Torsoåæ ‡
        
        Args:
            color_image: å½©è‰²å›¾åƒ
            depth_raw: åŸå§‹æ·±åº¦å›¾
            target_index: ç›®æ ‡åŒºåŸŸç¼–å· (0-35)
        
        Returns:
            DictåŒ…å«:
                - target_index: int
                - pixel_coord: (x, y)  # åƒç´ åæ ‡
                - depth_meters: float
                - camera_coord: [x, y, z]  # ç›¸æœºåæ ‡ç³»
                - torso_coord: [x, y, z]   # Torsoåæ ‡ç³»
                - screen_corners: [[x,y], ...]
                - target_region: {...}
        """
        # 1. è°ƒç”¨YOLOæœåŠ¡
        yolo_result = self.yolo_client.detect_screen_target(color_image, target_index)
        
        if not yolo_result or not yolo_result.get('found'):
            print(f"âŒ æœªæ£€æµ‹åˆ°å±å¹•æˆ–ç›®æ ‡åŒºåŸŸ")
            return None
        
        # 2. æå–ç›®æ ‡ä¸­å¿ƒç‚¹åƒç´ åæ ‡
        target_center = yolo_result['target_region']['center']
        pixel_x, pixel_y = target_center
        
        print(f"\nğŸ“ ç›®æ ‡åŒºåŸŸ {target_index} ä¸­å¿ƒ: ({pixel_x}, {pixel_y})")
        
        # 3. è·å–æ·±åº¦å€¼
        depth_value, search_offset = self.depth_helper.get_precise_depth(
            depth_raw, pixel_x, pixel_y, max_search_radius=20
        )
        
        if depth_value == 0:
            print(f"âŒ æ— æ³•è·å–æœ‰æ•ˆæ·±åº¦å€¼")
            return None
        
        # 4. è½¬æ¢ä¸ºç±³åˆ¶
        depth_meters = depth_value * self.camera.depth_scale
        
        if search_offset != (0, 0):
            print(f"ğŸ” æ·±åº¦æœç´¢åç§»: {search_offset}, æ·±åº¦: {depth_meters:.3f}m")
        else:
            print(f"ğŸ“ æ·±åº¦: {depth_meters:.3f}m")
        
        # 5. åæŠ•å½±åˆ°ç›¸æœºåæ ‡ç³»
        actual_pixel_x = pixel_x + search_offset[0]
        actual_pixel_y = pixel_y + search_offset[1]
        
        camera_point = rs.rs2_deproject_pixel_to_point(
            self.camera.depth_intrinsics,
            [actual_pixel_x, actual_pixel_y],
            depth_meters
        )
        
        # 6. è½¬æ¢åˆ°Torsoåæ ‡ç³»
        torso_point = self.coord_transformer.process(np.array(camera_point))
        
        print(f"ğŸ“· ç›¸æœºåæ ‡: X={camera_point[0]:.3f}, Y={camera_point[1]:.3f}, Z={camera_point[2]:.3f}")
        print(f"ğŸ¤– Torsoåæ ‡: X={torso_point[0]:.3f}, Y={torso_point[1]:.3f}, Z={torso_point[2]:.3f}")
        
        return {
            'target_index': target_index,
            'pixel_coord': (pixel_x, pixel_y),
            'actual_pixel_coord': (actual_pixel_x, actual_pixel_y),
            'depth_meters': float(depth_meters),
            'camera_coord': list(camera_point),  # âœ… ä¿®å¤: camera_point å·²ç»æ˜¯ list
            'torso_coord': torso_point.tolist(),  # âœ… torso_point æ˜¯ ndarray,éœ€è¦è½¬æ¢
            'screen_corners': yolo_result['screen_corners'],
            'target_region': yolo_result['target_region'],
            'search_offset': search_offset
        }
    
    def visualize_result(self, color_image: np.ndarray, result: Dict[str, Any]) -> np.ndarray:
        """åœ¨å›¾åƒä¸Šå¯è§†åŒ–æ£€æµ‹ç»“æœ"""
        vis_image = color_image.copy()
        
        # ç»˜åˆ¶å±å¹•å››è§’ç‚¹
        screen_corners = result['screen_corners']
        for i, corner in enumerate(screen_corners):
            cv2.circle(vis_image, tuple(corner), 5, (0, 255, 0), -1)
            cv2.putText(vis_image, str(i+1), (corner[0]+10, corner[1]-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        # ç»˜åˆ¶å±å¹•è¾¹æ¡†
        for i in range(4):
            pt1 = tuple(screen_corners[i])
            pt2 = tuple(screen_corners[(i+1) % 4])
            cv2.line(vis_image, pt1, pt2, (255, 0, 0), 2)
        
        # ç»˜åˆ¶ç›®æ ‡åŒºåŸŸ
        target_corners = result['target_region']['corners']
        for i in range(4):
            pt1 = tuple(target_corners[i])
            pt2 = tuple(target_corners[(i+1) % 4])
            cv2.line(vis_image, pt1, pt2, (0, 255, 255), 2)
        
        # ç»˜åˆ¶ç›®æ ‡ä¸­å¿ƒç‚¹
        center = result['pixel_coord']
        actual_center = result['actual_pixel_coord']
        
        cv2.drawMarker(vis_image, center, (0, 255, 255), 
                      cv2.MARKER_TILTED_CROSS, 15, 2)
        
        if result['search_offset'] != (0, 0):
            cv2.circle(vis_image, actual_center, 5, (0, 0, 255), -1)
            cv2.line(vis_image, center, actual_center, (255, 255, 0), 1)
        else:
            cv2.circle(vis_image, center, 5, (0, 255, 0), -1)
        
        # æ˜¾ç¤ºTorsoåæ ‡
        torso = result['torso_coord']
        label = f"Grid{result['target_index']}: X={torso[0]:.2f} Y={torso[1]:.2f} Z={torso[2]:.2f}"
        cv2.putText(vis_image, label, (actual_center[0]+12, actual_center[1]-5),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        return vis_image
    
    def run_interactive(self):
        """è¿è¡Œäº¤äº’å¼å®šä½æ¨¡å¼"""
        print("\nğŸš€ å¯åŠ¨å±å¹•ç›®æ ‡å®šä½å™¨")
        print("\næ“ä½œè¯´æ˜:")
        print("  0-9 - å¿«é€Ÿé€‰æ‹©ç›®æ ‡ç¼–å· (0-9)")
        print("  N - è¾“å…¥è‡ªå®šä¹‰ç¼–å· (0-35)")
        print("  SPACE - æ‰§è¡Œæ£€æµ‹å®šä½")
        print("  S - ä¿å­˜å½“å‰ç»“æœ")
        print("  Q/ESC - é€€å‡º")
        print("=" * 60)
        
        if not self.camera.start():
            print("âŒ æ‘„åƒå¤´å¯åŠ¨å¤±è´¥")
            return
        
        window_name = "Screen Target Locator"
        cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)
        
        try:
            while True:
                color_image, depth_raw, depth_colored = self.camera.get_frames()
                
                if color_image is None or depth_raw is None:
                    continue
                
                # æ˜¾ç¤ºå›¾åƒ
                if self.last_detection_result:
                    display_image = self.visualize_result(color_image, self.last_detection_result)
                else:
                    display_image = color_image.copy()
                
                # æ·»åŠ çŠ¶æ€ä¿¡æ¯
                status_text = f"ç›®æ ‡ç¼–å·: {self.current_target_index} | æŒ‰SPACEæ£€æµ‹"
                cv2.putText(display_image, status_text, (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                if self.last_torso_coords:
                    coord_text = f"Torso: {self.last_torso_coords}"
                    cv2.putText(display_image, coord_text, (10, 60),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                
                cv2.imshow(window_name, display_image)
                
                # é”®ç›˜æ§åˆ¶
                key = cv2.waitKey(1) & 0xFF
                
                if key in (27, ord('q')):  # ESC or Q
                    break
                    
                elif ord('0') <= key <= ord('9'):  # æ•°å­—é”®
                    self.current_target_index = key - ord('0')
                    print(f"\nğŸ¯ é€‰æ‹©ç›®æ ‡ç¼–å·: {self.current_target_index}")
                    
                elif key == ord('n'):  # è‡ªå®šä¹‰è¾“å…¥
                    try:
                        index = int(input("è¯·è¾“å…¥ç›®æ ‡ç¼–å· (0-35): "))
                        if 0 <= index <= 35:
                            self.current_target_index = index
                            print(f"ğŸ¯ é€‰æ‹©ç›®æ ‡ç¼–å·: {self.current_target_index}")
                        else:
                            print("âŒ ç¼–å·è¶…å‡ºèŒƒå›´")
                    except ValueError:
                        print("âŒ è¾“å…¥æ— æ•ˆ")
                    
                elif key == ord(' '):  # ç©ºæ ¼é”®æ£€æµ‹
                    print(f"\nğŸ” å¼€å§‹æ£€æµ‹ç›®æ ‡åŒºåŸŸ {self.current_target_index}...")
                    result = self.detect_and_locate(
                        color_image, depth_raw, self.current_target_index
                    )
                    if result:
                        self.last_detection_result = result
                        self.last_torso_coords = result['torso_coord']
                        print("âœ… æ£€æµ‹æˆåŠŸ")
                    else:
                        self.last_detection_result = None
                        self.last_torso_coords = None
                    
                elif key == ord('s'):  # ä¿å­˜ç»“æœ
                    if self.last_detection_result:
                        self._save_result(color_image, self.last_detection_result)
                    else:
                        print("âŒ æ— å¯ä¿å­˜çš„ç»“æœ")
        
        except KeyboardInterrupt:
            print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        finally:
            self.camera.stop()
            cv2.destroyAllWindows()
            print("[INFO] ç¨‹åºå·²é€€å‡º")
    
    def _save_result(self, color_image: np.ndarray, result: Dict[str, Any]):
        """ä¿å­˜æ£€æµ‹ç»“æœ"""
        output_dir = Path("data/screen_target_results")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        from datetime import datetime
        import json
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜å¯è§†åŒ–å›¾åƒ
        vis_image = self.visualize_result(color_image, result)
        img_path = output_dir / f"target_{result['target_index']}_{timestamp}.png"
        cv2.imwrite(str(img_path), vis_image)
        
        # ä¿å­˜JSONæ•°æ®
        json_path = output_dir / f"target_{result['target_index']}_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
        print(f"   å›¾åƒ: {img_path}")
        print(f"   æ•°æ®: {json_path}")


# ==========================================
# å‘½ä»¤è¡Œæ¥å£
# ==========================================
def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å±å¹•ç›®æ ‡å®šä½å™¨")
    parser.add_argument("--server", type=str, default="http://192.168.77.103:28000",
                       help="YOLOæœåŠ¡åœ°å€")
    parser.add_argument("--target", type=int, default=0,
                       help="ç›®æ ‡åŒºåŸŸç¼–å· (0-35)")
    parser.add_argument("--mode", type=str, default="interactive",
                       choices=["interactive", "single"],
                       help="è¿è¡Œæ¨¡å¼: interactive(äº¤äº’å¼) æˆ– single(å•æ¬¡æ£€æµ‹)")
    
    args = parser.parse_args()
    
    locator = ScreenTargetLocator(args.server)
    
    if args.mode == "interactive":
        locator.current_target_index = args.target
        locator.run_interactive()
    else:
        # å•æ¬¡æ£€æµ‹æ¨¡å¼
        if not locator.camera.start():
            print("âŒ æ‘„åƒå¤´å¯åŠ¨å¤±è´¥")
            return
        
        try:
            print("â³ ç­‰å¾…ç¨³å®šå¸§...")
            import time
            time.sleep(2)
            
            color_image, depth_raw, _ = locator.camera.get_frames()
            
            if color_image is not None and depth_raw is not None:
                result = locator.detect_and_locate(color_image, depth_raw, args.target)
                
                if result:
                    print("\nâœ… æ£€æµ‹æˆåŠŸ:")
                    print(f"   ç›®æ ‡ç¼–å·: {result['target_index']}")
                    print(f"   åƒç´ åæ ‡: {result['pixel_coord']}")
                    print(f"   Torsoåæ ‡: {result['torso_coord']}")
                    
                    # ä¿å­˜ç»“æœ
                    locator._save_result(color_image, result)
                else:
                    print("âŒ æ£€æµ‹å¤±è´¥")
            else:
                print("âŒ æ— æ³•è·å–å›¾åƒ")
                
        finally:
            locator.camera.stop()


if __name__ == "__main__":
    main()