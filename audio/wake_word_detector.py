import re
import time
import threading
from collections import deque
from difflib import SequenceMatcher
import jieba
from pypinyin import pinyin, Style
import numpy as np

class WakeWordDetector:
    def __init__(self, target_wake_word="å°å®‰", confidence_threshold=0.6, verbose=False):
        """
        :param verbose: æ˜¯å¦è¾“å‡ºåˆå§‹åŒ–å’Œæ£€æµ‹æ—¥å¿—
        """
        self.target_wake_word = target_wake_word
        self.confidence_threshold = confidence_threshold
        self.verbose = verbose
        
        # ç”Ÿæˆå˜ä½“
        self.wake_word_variants = self._generate_variants(target_wake_word)
        
        # æå–æ‹¼éŸ³
        self.target_pinyin = self._text_to_pinyin(target_wake_word)
        
        # å†·å´æœºåˆ¶
        self.last_wake_time = 0
        self.cooldown_period = 3.0
        self.detection_history = deque(maxlen=10)
        
        # âš¡ å…³é”®ä¿®å¤ï¼šé¢„çƒ­ jieba åˆ†è¯ï¼Œé¿å…ç¬¬ä¸€æ¬¡å”¤é†’æ—¶å¡é¡¿
        if self.verbose:
            print("â³ æ­£åœ¨é¢„çƒ­åˆ†è¯æ¨¡å‹...")
        list(jieba.cut("é¢„çƒ­åˆ†è¯"))
        
        # åªåœ¨ verbose=True æ—¶è¾“å‡º
        if self.verbose:
            print("ğŸ¯ å”¤é†’è¯æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
            print(f"   ç›®æ ‡å”¤é†’è¯: {target_wake_word}")
            print(f"   å˜ä½“æ•°é‡: {len(self.wake_word_variants)}")
            print(f"   ç›®æ ‡æ‹¼éŸ³: {self.target_pinyin}")
        
    def _generate_variants(self, wake_word):
        """ç”Ÿæˆå”¤é†’è¯çš„å¤šç§å˜ä½“"""
        variants = set()
        
        # 1. åŸå§‹è¯
        variants.add(wake_word)
        
        # 2. æ‹¼éŸ³ç›¸ä¼¼
        pinyin_variants = self._text_to_pinyin_variants(wake_word)
        variants.update(pinyin_variants)
        
        # 3. å¸¸è§é”™è¯» - æ ¹æ®å”¤é†’è¯åŠ¨æ€ç”Ÿæˆ
        common_misreads = ["å°å®‰", "å°æŒ‰", "å°æ¡ˆ", "å°æš—", "å°å²¸", "å°é", "å°ä¿º", "å°æ°¨", "å°åºµ", "å°è°™", "å°é“µ", "æ™“å®‰", "ç¬‘å®‰", "å°å•Š"]
        
        variants.update(common_misreads)
        
        # 4. æ‹†åˆ†ç»„åˆ
        for i in range(1, len(wake_word)):
            part1 = wake_word[:i]
            part2 = wake_word[i:]
            if part1 and part2:
                variants.add(part1 + part2)
                variants.add(part2 + part1)
        
        return list(variants)
    
    def _text_to_pinyin_variants(self, text):
        """è·å–æ–‡æœ¬çš„æ‹¼éŸ³å¤šç§å¯èƒ½å½¢å¼"""
        pinyin_list = pinyin(text, style=Style.NORMAL, heteronym=False)
        return [''.join(item[0] for item in pinyin_list)]
    
    def _text_to_pinyin(self, text):
        """è·å–æ–‡æœ¬çš„æ‹¼éŸ³å­—ç¬¦ä¸²"""
        try:
            py_list = pinyin(text, style=Style.NORMAL, heteronym=False)
            return ' '.join([item[0] for item in py_list])
        except:
            return text
    
    def _calculate_text_similarity(self, text1, text2):
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
        # 1. ç›´æ¥å­—ç¬¦ä¸²åŒ¹é…
        if text1 == text2:
            return 1.0
            
        # 2. ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
        edit_similarity = SequenceMatcher(None, text1, text2).ratio()
        
        # 3. æ‹¼éŸ³ç›¸ä¼¼åº¦ï¼ˆä¿®å¤ï¼šä½¿ç”¨ _text_to_pinyin æ–¹æ³•ï¼‰
        pinyin1 = self._text_to_pinyin(text1)
        pinyin2 = self._text_to_pinyin(text2)
        pinyin_similarity = SequenceMatcher(None, pinyin1, pinyin2).ratio()
        
        # 4. åŒ…å«å…³ç³»æ£€æŸ¥
        contains_score = 0.0
        if text2 in text1 or text1 in text2:
            contains_score = 0.3
            
        #ç»¼åˆè¯„åˆ† (æƒé‡å¯è°ƒ)
        final_score = (
            edit_similarity * 0.4 + 
            pinyin_similarity * 0.4 + 
            contains_score * 0.2
        )
        
        return final_score
    
    def _extract_wake_candidates(self, text):
        """ä»æ–‡æœ¬ä¸­æå–å¯èƒ½çš„å”¤é†’è¯å€™é€‰"""
        if not text or len(text.strip()) == 0:
            return []
            
        candidates = []
        text = text.strip()
        
        if self.verbose:
            print(f"ğŸ” æå–å€™é€‰è¯ï¼Œè¾“å…¥æ–‡æœ¬: '{text}'")
        
        # 1. ç›´æ¥æ£€æŸ¥å˜ä½“æ˜¯å¦åŒ…å«åœ¨æ–‡æœ¬ä¸­
        for variant in self.wake_word_variants:
            if variant in text:
                candidates.append(variant)
                if self.verbose:
                    print(f"   âœ… ç›´æ¥åŒ¹é…å˜ä½“: '{variant}'")
        
        # 2. æ»‘åŠ¨çª—å£æå–æ‰€æœ‰ä¸¤å­—ç»„åˆ
        if len(text) >= 2:
            for i in range(len(text) - 1):
                two_char = text[i:i+2]
                # ç¡®ä¿æ˜¯ä¸¤ä¸ªæœ‰æ•ˆå­—ç¬¦
                if len(two_char) == 2 and not re.search(r'[a-zA-Z0-9\s\.,!?ï¼Œã€‚ï¼ï¼Ÿ]', two_char):
                    candidates.append(two_char)
                    if self.verbose:
                        print(f"   ğŸ”¸ æ»‘åŠ¨çª—å£æå–: '{two_char}'")
        
        # 3. åˆ†è¯åæ£€æŸ¥ç›¸é‚»è¯ç»„åˆ
        try:
            words = list(jieba.cut(text, cut_all=False))
            if self.verbose:
                print(f"   ğŸ“ åˆ†è¯ç»“æœ: {words}")
            
            # æ£€æŸ¥å•ä¸ªè¯
            for word in words:
                if len(word) == 2 and not re.search(r'[a-zA-Z0-9\s\.,!?ï¼Œã€‚ï¼ï¼Ÿ]', word):
                    candidates.append(word)
                    if self.verbose:
                        print(f"   ğŸ”¹ åˆ†è¯å•è¯: '{word}'")
            
            # æ£€æŸ¥ç›¸é‚»è¯ç»„åˆ
            for i in range(len(words) - 1):
                combined = words[i] + words[i + 1]
                if len(combined) == 2 and not re.search(r'[a-zA-Z0-9\s\.,!?ï¼Œã€‚ï¼ï¼Ÿ]', combined):
                    candidates.append(combined)
                    if self.verbose:
                        print(f"   ğŸ”¹ åˆ†è¯ç»„åˆ: '{combined}'")
                    
        except Exception as e:
            if self.verbose:
                print(f"   âŒ åˆ†è¯é”™è¯¯: {e}")
        
        # 4. å»é™¤æ ‡ç‚¹å’Œç©ºæ ¼åå†æ¬¡æå–
        clean_text = re.sub(r'[a-zA-Z0-9\s\.,!?ï¼Œã€‚ï¼ï¼Ÿ]', '', text)
        if clean_text and len(clean_text) >= 2:
            for i in range(len(clean_text) - 1):
                two_char = clean_text[i:i+2]
                if len(two_char) == 2:
                    candidates.append(two_char)
                    if self.verbose:
                        print(f"   ğŸ§¹ æ¸…ç†åæå–: '{two_char}'")
        
        # å»é‡å¹¶è¿”å›
        unique_candidates = list(set(candidates))
        if self.verbose:
            print(f"   ğŸ¯ æœ€ç»ˆå€™é€‰è¯: {unique_candidates}")
        
        return unique_candidates
    
    def detect_wake_word(self, recognized_text):
        """æ£€æµ‹å”¤é†’è¯"""
        current_time = time.time()
        
        if self.verbose:
            print(f"\nğŸ” å¼€å§‹æ£€æµ‹å”¤é†’è¯: '{recognized_text}'")
        
        # æ£€æŸ¥å†·å´æ—¶é—´
        if current_time - self.last_wake_time < self.cooldown_period:
            if self.verbose:
                remaining_cooldown = self.cooldown_period - (current_time - self.last_wake_time)
                print(f"â³ å†·å´æ—¶é—´æœªç»“æŸï¼Œå‰©ä½™: {remaining_cooldown:.1f}ç§’")
            return False, 0.0, ""
            
        if not recognized_text or len(recognized_text.strip()) == 0:
            if self.verbose:
                print("âŒ è¾“å…¥æ–‡æœ¬ä¸ºç©º")
            return False, 0.0, ""
            
        # æ¸…ç†æ–‡æœ¬
        text = recognized_text.strip().replace(" ", "").replace("ï¼Œ", "").replace("ã€‚", "")
        if self.verbose:
            print(f"ğŸ“ æ¸…ç†åæ–‡æœ¬: '{text}'")
        
        # æ·»åŠ åˆ°æ–‡æœ¬ç¼“å†²åŒº
        self.detection_history.append((current_time, text))
        
        # æ£€æŸ¥å½“å‰æ–‡æœ¬
        candidates = self._extract_wake_candidates(text)
        
        best_score = 0.0
        best_candidate = ""
        
        if self.verbose:
            print(f"ğŸ¯ å¼€å§‹ç›¸ä¼¼åº¦è®¡ç®—...")
        for candidate in candidates:
            score = self._calculate_text_similarity(candidate, self.target_wake_word)
            if self.verbose:
                print(f"   å€™é€‰è¯: '{candidate}' â†’ ç›¸ä¼¼åº¦: {score:.3f}")
            if score > best_score:
                best_score = score
                best_candidate = candidate
        
        # æ”¹è¿›çš„å†å²ç»„åˆæ£€æµ‹é€»è¾‘
        if best_score < self.confidence_threshold:
            if self.verbose:
                print(f"ğŸ”„ å½“å‰æœ€é«˜åˆ† {best_score:.3f} < é˜ˆå€¼ {self.confidence_threshold}ï¼Œå°è¯•è¿‘æœŸç»„åˆ...")
            
            # è·å–æœ€è¿‘çš„æ–‡æœ¬ï¼Œä½†é™åˆ¶æ—¶é—´çª—å£å’Œç»„åˆæ–¹å¼
            recent_entries = list(self.detection_history)[-3:]  # æœ€è¿‘3æ¡
            
            # åªæœ‰åœ¨æ—¶é—´é—´éš”åˆç†çš„æƒ…å†µä¸‹æ‰è¿›è¡Œç»„åˆï¼ˆæ¯”å¦‚5ç§’å†…ï¼‰
            valid_entries = []
            for timestamp, old_text in recent_entries:
                if current_time - timestamp <= 5.0:  # 5ç§’æ—¶é—´çª—å£
                    valid_entries.append((timestamp, old_text))
            
            if len(valid_entries) >= 2:  # è‡³å°‘éœ€è¦2æ¡è®°å½•æ‰è¿›è¡Œç»„åˆ
                # å°è¯•ä¸åŒçš„ç»„åˆæ–¹å¼
                combined_candidates = []
                
                # 1. åªç»„åˆç›¸é‚»çš„çŸ­æ–‡æœ¬ï¼ˆæ¯ä¸ªæ–‡æœ¬é•¿åº¦<=4ä¸ªå­—ç¬¦ï¼‰
                short_texts = [text for _, text in valid_entries if len(text) <= 4]
                if len(short_texts) >= 2:
                    combined_short = "".join(short_texts[-2:])  # æœ€è¿‘çš„ä¸¤ä¸ªçŸ­æ–‡æœ¬
                    if self.verbose:
                        print(f"ğŸ”— çŸ­æ–‡æœ¬ç»„åˆ: '{combined_short}'")
                    combined_candidates.extend(self._extract_wake_candidates(combined_short))
                
                # 2. æ£€æŸ¥æ˜¯å¦å­˜åœ¨å•å­—ç¬¦åŒ¹é…ï¼ˆå¦‚"å°"+"å®‰"çš„åˆ†å‰²æƒ…å†µï¼‰
                for i in range(len(valid_entries) - 1):
                    text1 = valid_entries[i][1]
                    text2 = valid_entries[i + 1][1]
                    
                    # åªæœ‰å½“ä¸¤ä¸ªæ–‡æœ¬éƒ½å¾ˆçŸ­æ—¶æ‰ç»„åˆ
                    if len(text1) <= 3 and len(text2) <= 3:
                        mini_combined = text1 + text2
                        if self.verbose:
                            print(f"ğŸ”— çŸ­å¥ç»„åˆ: '{text1}' + '{text2}' = '{mini_combined}'")
                        combined_candidates.extend(self._extract_wake_candidates(mini_combined))
                
                # æ£€æŸ¥ç»„åˆå€™é€‰è¯
                if combined_candidates:
                    if self.verbose:
                        print(f"ğŸ“ å†å²ç»„åˆå€™é€‰è¯: {list(set(combined_candidates))}")
                    for candidate in set(combined_candidates):  # å»é‡
                        score = self._calculate_text_similarity(candidate, self.target_wake_word)
                        if self.verbose:
                            print(f"   å†å²å€™é€‰è¯: '{candidate}' â†’ ç›¸ä¼¼åº¦: {score:.3f}")
                        if score > best_score:
                            best_score = score
                            best_candidate = candidate
                else:
                    if self.verbose:
                        print("ğŸ“ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å†å²ç»„åˆå€™é€‰è¯")
            else:
                if self.verbose:
                    print("ğŸ“ å†å²è®°å½•ä¸è¶³æˆ–æ—¶é—´çª—å£è¶…å‡ºï¼Œè·³è¿‡ç»„åˆæ£€æµ‹")
        
        # åˆ¤æ–­æ˜¯å¦å”¤é†’
        if best_score >= self.confidence_threshold:
            self.last_wake_time = current_time
            if self.verbose:
                print(f"âœ… å”¤é†’æˆåŠŸï¼æœ€ä½³å€™é€‰: '{best_candidate}', å¾—åˆ†: {best_score:.3f}")
            
            # å”¤é†’æˆåŠŸåæ¸…ç©ºå†å²ç¼“å†²åŒºï¼Œé¿å…å½±å“åç»­æ£€æµ‹
            self.detection_history.clear()
            
            return True, best_score, best_candidate
        else:
            if self.verbose:
                print(f"âŒ æœªè¾¾åˆ°å”¤é†’é˜ˆå€¼ï¼Œæœ€ä½³å¾—åˆ†: {best_score:.3f}")
            
        return False, best_score, best_candidate
    
    def reset_cooldown(self):
        """é‡ç½®å†·å´æ—¶é—´ï¼ˆç”¨äºæµ‹è¯•æˆ–æ‰‹åŠ¨é‡ç½®ï¼‰"""
        self.last_wake_time = 0
        # æµ‹è¯•æ—¶ä¹Ÿæ¸…ç©ºç¼“å†²åŒº
        self.detection_history.clear()
        if self.verbose:
            print("ğŸ”„ å†·å´æ—¶é—´å’Œå†å²ç¼“å†²åŒºå·²é‡ç½®")
    
    def get_debug_info(self):
        """è·å–è°ƒè¯•ä¿¡æ¯"""
        return {
            "buffer_size": len(self.detection_history),
            "last_wake_time": self.last_wake_time,
            "target_pinyin": self.target_pinyin,
            "recent_texts": [item[1] for item in list(self.detection_history)[-5:]],
            "wake_variants": self.wake_word_variants[:10]  # åªæ˜¾ç¤ºå‰10ä¸ªå˜ä½“
        }

# æµ‹è¯•ä»£ç 
def test_wake_word_detector():
    """æµ‹è¯•å”¤é†’è¯æ£€æµ‹å™¨"""
    detector = WakeWordDetector("å°å®‰", confidence_threshold=0.6, verbose=True)  # æµ‹è¯•æ—¶å¼€å¯è¯¦ç»†æ—¥å¿—
    
    test_cases = [
        "å°å®‰ä½ å¥½",
        "å°æŒ‰ï¼Œå¸®æˆ‘ä¸€ä¸‹", 
        "æˆ‘æƒ³å«å°æ¡ˆè¿‡æ¥",
        "å°æš—è¾¹æœ‰ä»€ä¹ˆ",
        "helloå°å®‰world",
        "å°æœ‹å‹ä½ å¥½",
        "ä»Šå¤©å¤©æ°”ä¸é”™",
        "å° å®‰",
        "å°å®‰å°å®‰",
        "å°å²¸æœ‰ä»€ä¹ˆç”¨",
        "æ™“å®‰å¿«æ¥"
    ]
    
    print("ğŸ§ª æµ‹è¯•å”¤é†’è¯æ£€æµ‹å™¨")
    print(f"ç›®æ ‡å”¤é†’è¯: å°å®‰")
    print(f"ç½®ä¿¡åº¦é˜ˆå€¼: {detector.confidence_threshold}")
    print("=" * 60)
    
    for i, test_text in enumerate(test_cases, 1):
        print(f"\nğŸ“‹ æµ‹è¯• {i}/{len(test_cases)} - ç‹¬ç«‹æµ‹è¯•")
        
        # æ¯æ¬¡æµ‹è¯•å‰å®Œå…¨é‡ç½®çŠ¶æ€
        detector.reset_cooldown()
        
        is_wake, confidence, candidate = detector.detect_wake_word(test_text)
        
        status = "âœ… å”¤é†’" if is_wake else "âŒ æœªå”¤é†’"
        print(f"ğŸ† ç»“æœ: {status} | ç½®ä¿¡åº¦: {confidence:.3f} | å€™é€‰è¯: '{candidate}' | åŸæ–‡: '{test_text}'")
        
        print("-" * 40)
    
    # æ˜¾ç¤ºæœ€ç»ˆè°ƒè¯•ä¿¡æ¯
    print("\nğŸ“Š æœ€ç»ˆè°ƒè¯•ä¿¡æ¯:")
    debug_info = detector.get_debug_info()
    for key, value in debug_info.items():
        print(f"  {key}: {value}")

if __name__ == "__main__":
    test_wake_word_detector()