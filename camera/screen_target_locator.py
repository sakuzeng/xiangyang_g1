#!/usr/bin/env python3
"""
screen_target_locator.py
========================

åŸºäºYOLOæ£€æµ‹çš„å±å¹•ç›®æ ‡å®šä½ç³»ç»Ÿ

åŠŸèƒ½:
1. è°ƒç”¨å¤–éƒ¨YOLOæœåŠ¡è¯†åˆ«å±å¹•åŒºåŸŸ
2. æ ¹æ®æŒ‡å®šç¼–å·è·å–ç›®æ ‡åŒºåŸŸä¸­å¿ƒç‚¹
3. å°†åƒç´ åæ ‡è½¬æ¢ä¸ºæœºå™¨äººTorsoåæ ‡ç³» (ğŸ†• æ”¯æŒTorso ZéªŒè¯)
4. æ”¯æŒå®æ—¶é¢„è§ˆå’Œäº¤äº’å¼æ“ä½œ

å•ä¸€æ£€æµ‹å‘½ä»¤è¡Œç¤ºä¾‹:
python screen_target_locator.py --mode single --target 17
"""

import sys
import os
import cv2
import numpy as np
import requests
import pyrealsense2 as rs
from scipy.spatial.transform import Rotation as R
from typing import Optional, Tuple, Dict, Any
from pathlib import Path

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'loco', 'unitree_sdk_python'))
from unitree_sdk2py.camera.realsense_camera_client import RealSenseCamera


# ==========================================
# åæ ‡è½¬æ¢å™¨
# ==========================================
class CoordTransformer:
    """ç›¸æœºåæ ‡ç³» -> Torsoåæ ‡ç³»è½¬æ¢å™¨"""
    
    def __init__(self):
        self.urdf_trans = np.array([0.0576235, 0.01753, 0.42987])
        self.pitch_offset = 0.23
        self.base_pitch = 0.8307767239493009
        self._recalc_matrices()
    
    def _recalc_matrices(self):
        final_pitch = self.base_pitch + self.pitch_offset
        self.mat_opt_to_link = np.array([[0, 0, 1], [-1, 0, 0], [0, -1, 0]])
        self.urdf_rpy = [0, final_pitch, 0]
        r_obj = R.from_euler('xyz', self.urdf_rpy, degrees=False)
        try:
            self.mat_link_to_torso = r_obj.as_matrix()
        except:
            self.mat_link_to_torso = r_obj.as_dcm()
    
    def process(self, point_cam_optical: np.ndarray) -> np.ndarray:
        P_opt = np.array(point_cam_optical)
        P_link = self.mat_opt_to_link @ P_opt
        P_torso = self.mat_link_to_torso @ P_link + self.urdf_trans
        return P_torso


# ==========================================
# YOLO æœåŠ¡å®¢æˆ·ç«¯
# ==========================================
class YOLOClient:
    """YOLOå±å¹•æ£€æµ‹æœåŠ¡å®¢æˆ·ç«¯"""
    
    def __init__(self, server_url: str = "http://192.168.77.103:28000"):
        self.server_url = server_url.rstrip('/')
        self.endpoint = f"{self.server_url}/yolo"
    
    def detect_screen_target(self, image: np.ndarray, target_index: int) -> Optional[Dict[str, Any]]:
        try:
            _, img_encoded = cv2.imencode('.jpg', image, [cv2.IMWRITE_JPEG_QUALITY, 95])
            files = {'file': ('image.jpg', img_encoded.tobytes(), 'image/jpeg')}
            data = {'target_index': target_index}
            
            response = requests.post(self.endpoint, files=files, data=data, timeout=5.0)
            
            if response.status_code == 200:
                return response.json()
            else:
                print(f"âŒ YOLOæœåŠ¡é”™è¯¯: {response.status_code}")
                return None
        except requests.exceptions.Timeout:
            print("âŒ YOLOæœåŠ¡è¶…æ—¶")
            return None
        except Exception as e:
            print(f"âŒ YOLOæœåŠ¡è°ƒç”¨å¤±è´¥: {e}")
            return None


# ==========================================
# ğŸ†• å‡çº§ç‰ˆæ·±åº¦è¾…åŠ©å·¥å…·
# ==========================================
class DepthHelper:
    """æ·±åº¦å›¾è¾…åŠ©å·¥å…· (æ”¯æŒTorso ZéªŒè¯)"""
    
    def __init__(self, 
                 coord_transformer: CoordTransformer,
                 camera_intrinsics,
                 depth_scale: float,
                 expected_torso_z: float = -0.17,
                 torso_z_tolerance: float = 0.05):
        """
        Args:
            coord_transformer: åæ ‡è½¬æ¢å™¨
            camera_intrinsics: ç›¸æœºå†…å‚
            depth_scale: æ·±åº¦æ¯”ä¾‹
            expected_torso_z: é¢„æœŸTorso ZåŸºå‡†å€¼ (ç±³)
            torso_z_tolerance: Zå€¼å®¹å·® (ç±³)
        """
        self.transformer = coord_transformer
        self.intrinsics = camera_intrinsics
        self.depth_scale = depth_scale
        self.expected_torso_z = expected_torso_z
        self.torso_z_tolerance = torso_z_tolerance
    
    def _is_torso_z_reasonable(self, torso_z: float) -> bool:
        """æ£€æŸ¥Torso Zå€¼æ˜¯å¦åˆç†"""
        deviation = abs(torso_z - self.expected_torso_z)
        return deviation <= self.torso_z_tolerance
    
    def get_precise_depth_basic(self, depth_image: np.ndarray, x: int, y: int, 
                               max_search_radius: int = 20) -> Tuple[float, Tuple[int, int]]:
        """
        åŸºç¡€åŒå¿ƒåœ†æœç´¢ (æ— Torso ZéªŒè¯)
        
        Returns:
            (depth_value, (offset_x, offset_y))
        """
        height, width = depth_image.shape
        
        if not (0 <= x < width and 0 <= y < height):
            return 0, (0, 0)
        
        # ç­–ç•¥1: ä¸­å¿ƒç‚¹
        center_depth = depth_image[y, x]
        if center_depth > 0:
            return center_depth, (0, 0)
        
        # ç­–ç•¥2: åŒå¿ƒåœ†æœç´¢
        for radius in range(1, max_search_radius + 1):
            candidates = []
            num_samples = max(8, radius * 2)
            
            for i in range(num_samples):
                angle = 2 * np.pi * i / num_samples
                dx = int(radius * np.cos(angle))
                dy = int(radius * np.sin(angle))
                nx, ny = x + dx, y + dy
                
                if 0 <= nx < width and 0 <= ny < height:
                    depth_val = depth_image[ny, nx]
                    if depth_val > 0:
                        candidates.append((depth_val, dx, dy, radius))
            
            if candidates:
                depths = [c[0] for c in candidates]
                median_depth = np.median(depths)
                best_candidate = min(candidates, key=lambda c: abs(c[0] - median_depth))
                depth_val, dx, dy, r = best_candidate
                return depth_val, (dx, dy)
        
        return 0, (0, 0)
    
    def collect_valid_depth_candidates(self, depth_image: np.ndarray, x: int, y: int, 
                                      max_radius: int = 50) -> list:
        """
        ğŸ†• æ”¶é›†å‘¨å›´é€šè¿‡Torso ZéªŒè¯çš„æ·±åº¦ç‚¹
        
        Returns:
            list: [(depth_m, u, v), ...]
        """
        height, width = depth_image.shape
        valid_candidates = []
        
        for radius in range(1, max_radius + 1):
            num_samples = max(16, radius * 3)
            
            for i in range(num_samples):
                angle = 2 * np.pi * i / num_samples
                dx = int(radius * np.cos(angle))
                dy = int(radius * np.sin(angle))
                nx, ny = x + dx, y + dy
                
                if 0 <= nx < width and 0 <= ny < height:
                    depth_raw = depth_image[ny, nx]
                    if depth_raw > 0:
                        depth_m = depth_raw * self.depth_scale
                        
                        # ğŸ†• éªŒè¯Torso Z
                        pt_cam = rs.rs2_deproject_pixel_to_point(
                            self.intrinsics, [nx, ny], depth_m
                        )
                        pt_torso = self.transformer.process(pt_cam)
                        
                        if self._is_torso_z_reasonable(pt_torso[2]):
                            valid_candidates.append((depth_m, nx, ny))
            
            if len(valid_candidates) >= 8:
                break
        
        return valid_candidates
    
    def get_depth_with_validation(self, depth_image: np.ndarray, x: int, y: int, 
                                  initial_radius: int = 20, 
                                  max_radius: int = 50) -> Optional[Dict[str, Any]]:
        """
        ğŸ†• å¸¦Torso ZéªŒè¯çš„æ·±åº¦è·å–
        
        æµç¨‹:
        1. å¸¸è§„æœç´¢ â†’ Torso ZéªŒè¯
        2. å¤±è´¥åˆ™æ”¶é›†æ­£å¸¸æ·±åº¦ç‚¹ â†’ å–ä¸­å€¼
        
        Returns:
            DictåŒ…å«:
                - depth_meters: float
                - actual_pixel: (x, y)
                - torso_coord: [x, y, z]
                - search_offset: (dx, dy)
                - method: 'direct' æˆ– 'median_fill'
        """
        height, width = depth_image.shape
        
        if not (0 <= x < width and 0 <= y < height):
            return None
        
        # ========== é˜¶æ®µ1: å¸¸è§„æœç´¢ ==========
        depth_value, search_offset = self.get_precise_depth_basic(
            depth_image, x, y, max_search_radius=initial_radius
        )
        
        if depth_value > 0:
            dist = depth_value * self.depth_scale
            actual_u = x + search_offset[0]
            actual_v = y + search_offset[1]
            
            pt_cam = rs.rs2_deproject_pixel_to_point(
                self.intrinsics, [actual_u, actual_v], dist
            )
            pt_torso = self.transformer.process(pt_cam)
            
            if self._is_torso_z_reasonable(pt_torso[2]):
                return {
                    'depth_meters': dist,
                    'actual_pixel': (actual_u, actual_v),
                    'torso_coord': pt_torso,
                    'search_offset': search_offset,
                    'method': 'direct',
                    'torso_z_deviation': abs(pt_torso[2] - self.expected_torso_z)
                }
            else:
                print(f"  âš ï¸  å¸¸è§„æ·±åº¦å¼‚å¸¸ (Torso Z={pt_torso[2]:.3f}m),æ‰©å¤§æœç´¢...")
        
        # ========== é˜¶æ®µ2: ä¸­å€¼å¡«è¡¥ ==========
        print(f"  â†’ æ”¶é›†å‘¨å›´æ­£å¸¸æ·±åº¦ç‚¹ (åŠå¾„â‰¤{max_radius}px)...")
        valid_candidates = self.collect_valid_depth_candidates(
            depth_image, x, y, max_radius=max_radius
        )
        
        if len(valid_candidates) < 3:
            print(f"  âŒ æ­£å¸¸æ·±åº¦ç‚¹ä¸è¶³ ({len(valid_candidates)} < 3)")
            return None
        
        # å–ä¸­å€¼æ·±åº¦
        depths = [c[0] for c in valid_candidates]
        median_depth = np.median(depths)
        
        print(f"  âœ… æ‰¾åˆ° {len(valid_candidates)} ä¸ªæ­£å¸¸ç‚¹,ä¸­å€¼æ·±åº¦: {median_depth:.3f}m")
        
        # ä½¿ç”¨ç›®æ ‡ç‚¹åƒç´  + ä¸­å€¼æ·±åº¦
        pt_cam = rs.rs2_deproject_pixel_to_point(
            self.intrinsics, [x, y], median_depth
        )
        pt_torso = self.transformer.process(pt_cam)
        
        return {
            'depth_meters': median_depth,
            'actual_pixel': (x, y),
            'torso_coord': pt_torso,
            'search_offset': (0, 0),
            'method': 'median_fill',
            'num_valid_points': len(valid_candidates),
            'torso_z_deviation': abs(pt_torso[2] - self.expected_torso_z)
        }


# ==========================================
# ä¸»åº”ç”¨ç±»
# ==========================================
class ScreenTargetLocator:
    """å±å¹•ç›®æ ‡å®šä½å™¨"""
    
    def __init__(self, 
                 yolo_server_url: str = "http://192.168.77.103:28000",
                 expected_torso_z: float = -0.17,
                 torso_z_tolerance: float = 0.05):
        """
        Args:
            yolo_server_url: YOLOæœåŠ¡åœ°å€
            expected_torso_z: å±å¹•å¹³é¢çš„Torso ZåŸºå‡†å€¼ (ç±³)
            torso_z_tolerance: Zå€¼å®¹å·® (ç±³)
        """
        # ç»„ä»¶åˆå§‹åŒ–
        self.camera = RealSenseCamera(width=848, height=480, fps=30)
        self.yolo_client = YOLOClient(yolo_server_url)
        self.coord_transformer = CoordTransformer()
        
        # ğŸ†• ç­‰å¾…ç›¸æœºå¯åŠ¨åå†åˆå§‹åŒ– DepthHelper
        self.depth_helper = None
        self.expected_torso_z = expected_torso_z
        self.torso_z_tolerance = torso_z_tolerance
        
        # çŠ¶æ€å˜é‡
        self.current_target_index = 0
        self.last_detection_result = None
        self.last_torso_coords = None
        
        print(f"âœ… å±å¹•ç›®æ ‡å®šä½å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   YOLOæœåŠ¡: {yolo_server_url}")
        print(f"   æ‘„åƒå¤´åˆ†è¾¨ç‡: 848x480")
        print(f"   ğŸ†• Torso ZåŸºå‡†: {expected_torso_z:.3f}m (Â±{torso_z_tolerance*100:.0f}cm)")
    
    def _init_depth_helper(self):
        """åˆå§‹åŒ–æ·±åº¦è¾…åŠ©å·¥å…· (éœ€è¦ç›¸æœºå·²å¯åŠ¨)"""
        if self.depth_helper is None:
            self.depth_helper = DepthHelper(
                coord_transformer=self.coord_transformer,
                camera_intrinsics=self.camera.depth_intrinsics,
                depth_scale=self.camera.depth_scale,
                expected_torso_z=self.expected_torso_z,
                torso_z_tolerance=self.torso_z_tolerance
            )
    
    def detect_and_locate(self, color_image: np.ndarray, depth_raw: np.ndarray, 
                         target_index: int) -> Optional[Dict[str, Any]]:
        """
        æ£€æµ‹å±å¹•å¹¶å®šä½ç›®æ ‡åŒºåŸŸçš„Torsoåæ ‡
        
        Returns:
            DictåŒ…å«:
                - target_index: int
                - pixel_coord: (x, y)
                - depth_meters: float
                - camera_coord: [x, y, z]
                - torso_coord: [x, y, z]
                - ğŸ†• method: 'direct' æˆ– 'median_fill'
                - ğŸ†• torso_z_deviation: float
        """
        # 1. è°ƒç”¨YOLOæœåŠ¡
        yolo_result = self.yolo_client.detect_screen_target(color_image, target_index)
        
        if not yolo_result or not yolo_result.get('found'):
            print(f"âŒ æœªæ£€æµ‹åˆ°å±å¹•æˆ–ç›®æ ‡åŒºåŸŸ")
            return None
        
        # 2. æå–ç›®æ ‡ä¸­å¿ƒç‚¹
        target_center = yolo_result['target_region']['center']
        pixel_x, pixel_y = target_center
        
        print(f"\nğŸ“ ç›®æ ‡åŒºåŸŸ {target_index} ä¸­å¿ƒ: ({pixel_x}, {pixel_y})")
        
        # 3. ğŸ†• ä½¿ç”¨å‡çº§ç‰ˆæ·±åº¦è·å–
        depth_result = self.depth_helper.get_depth_with_validation(
            depth_raw, pixel_x, pixel_y,
            initial_radius=20, max_radius=50
        )
        
        if depth_result is None:
            print(f"âŒ æ— æ³•è·å–æœ‰æ•ˆæ·±åº¦å€¼")
            return None
        
        # 4. æå–ç»“æœ
        depth_meters = depth_result['depth_meters']
        actual_pixel = depth_result['actual_pixel']
        torso_point = depth_result['torso_coord']
        method = depth_result['method']
        
        print(f"ğŸ“ æ·±åº¦: {depth_meters:.3f}m (æ–¹æ³•: {method})")
        
        if method == 'median_fill':
            print(f"   åŸºäº {depth_result['num_valid_points']} ä¸ªæ­£å¸¸ç‚¹çš„ä¸­å€¼")
        
        # 5. è®¡ç®—ç›¸æœºåæ ‡
        camera_point = rs.rs2_deproject_pixel_to_point(
            self.camera.depth_intrinsics,
            list(actual_pixel),
            depth_meters
        )
        
        print(f"ğŸ“· ç›¸æœºåæ ‡: X={camera_point[0]:.3f}, Y={camera_point[1]:.3f}, Z={camera_point[2]:.3f}")
        print(f"ğŸ¤– Torsoåæ ‡: X={torso_point[0]:.3f}, Y={torso_point[1]:.3f}, Z={torso_point[2]:.3f}")
        print(f"ğŸ“Š Torso Zåå·®: {depth_result['torso_z_deviation']*100:.1f}cm")
        
        return {
            'target_index': target_index,
            'pixel_coord': (pixel_x, pixel_y),
            'actual_pixel_coord': actual_pixel,
            'depth_meters': float(depth_meters),
            'camera_coord': list(camera_point),
            'torso_coord': torso_point.tolist(),
            'screen_corners': yolo_result['screen_corners'],
            'target_region': yolo_result['target_region'],
            'search_offset': depth_result['search_offset'],
            'method': method,  # ğŸ†•
            'torso_z_deviation': depth_result['torso_z_deviation']  # ğŸ†•
        }
    
    def visualize_result(self, color_image: np.ndarray, result: Dict[str, Any]) -> np.ndarray:
        """åœ¨å›¾åƒä¸Šå¯è§†åŒ–æ£€æµ‹ç»“æœ"""
        vis_image = color_image.copy()
        
        # ç»˜åˆ¶å±å¹•å››è§’ç‚¹
        screen_corners = result['screen_corners']
        for i, corner in enumerate(screen_corners):
            cv2.circle(vis_image, tuple(corner), 5, (0, 255, 0), -1)
            cv2.putText(vis_image, str(i+1), (corner[0]+10, corner[1]-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        # ç»˜åˆ¶å±å¹•è¾¹æ¡†
        for i in range(4):
            pt1 = tuple(screen_corners[i])
            pt2 = tuple(screen_corners[(i+1) % 4])
            cv2.line(vis_image, pt1, pt2, (255, 0, 0), 2)
        
        # ç»˜åˆ¶ç›®æ ‡åŒºåŸŸ
        target_corners = result['target_region']['corners']
        for i in range(4):
            pt1 = tuple(target_corners[i])
            pt2 = tuple(target_corners[(i+1) % 4])
            cv2.line(vis_image, pt1, pt2, (0, 255, 255), 2)
        
        # ç»˜åˆ¶ç›®æ ‡ä¸­å¿ƒç‚¹
        center = result['pixel_coord']
        actual_center = result['actual_pixel_coord']
        
        # ğŸ†• æ ¹æ®æ–¹æ³•é€‰æ‹©é¢œè‰²
        method = result.get('method', 'direct')
        if method == 'median_fill':
            marker_color = (255, 165, 0)  # æ©™è‰²
            marker_type = cv2.MARKER_DIAMOND
        else:
            marker_color = (0, 255, 0)  # ç»¿è‰²
            marker_type = cv2.MARKER_TILTED_CROSS
        
        cv2.drawMarker(vis_image, center, (0, 255, 255), 
                      cv2.MARKER_TILTED_CROSS, 15, 2)
        
        if result['search_offset'] != (0, 0):
            cv2.circle(vis_image, actual_center, 5, (0, 0, 255), -1)
            cv2.line(vis_image, center, actual_center, (255, 255, 0), 1)
        else:
            cv2.drawMarker(vis_image, center, marker_color, marker_type, 10, 2)
        
        # æ˜¾ç¤ºTorsoåæ ‡
        torso = result['torso_coord']
        z_dev = result.get('torso_z_deviation', 0) * 100
        label = f"Grid{result['target_index']}: Z={torso[2]:.2f}m (Â±{z_dev:.0f}cm)"
        cv2.putText(vis_image, label, (actual_center[0]+12, actual_center[1]-5),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, marker_color, 2)
        
        return vis_image
    
    def run_interactive(self):
        """è¿è¡Œäº¤äº’å¼å®šä½æ¨¡å¼"""
        print("\nğŸš€ å¯åŠ¨å±å¹•ç›®æ ‡å®šä½å™¨")
        print("\næ“ä½œè¯´æ˜:")
        print("  0-9 - å¿«é€Ÿé€‰æ‹©ç›®æ ‡ç¼–å· (0-9)")
        print("  N - è¾“å…¥è‡ªå®šä¹‰ç¼–å· (0-35)")
        print("  SPACE - æ‰§è¡Œæ£€æµ‹å®šä½")
        print("  S - ä¿å­˜å½“å‰ç»“æœ")
        print("  +/- - è°ƒæ•´Zå®¹å·®")
        print("  [/] - å¾®è°ƒZåŸºå‡†")
        print("  Q/ESC - é€€å‡º")
        print("=" * 60)
        
        if not self.camera.start():
            print("âŒ æ‘„åƒå¤´å¯åŠ¨å¤±è´¥")
            return
        
        # ğŸ†• åˆå§‹åŒ–DepthHelper
        self._init_depth_helper()
        
        window_name = "Screen Target Locator"
        cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)
        
        try:
            while True:
                color_image, depth_raw, depth_colored = self.camera.get_frames()
                
                if color_image is None or depth_raw is None:
                    continue
                
                # æ˜¾ç¤ºå›¾åƒ
                if self.last_detection_result:
                    display_image = self.visualize_result(color_image, self.last_detection_result)
                else:
                    display_image = color_image.copy()
                
                # æ·»åŠ çŠ¶æ€ä¿¡æ¯
                status_text = f"Target:{self.current_target_index} | Z:{self.expected_torso_z:.2f}m(Â±{self.torso_z_tolerance*100:.0f}cm)"
                cv2.putText(display_image, status_text, (10, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                if self.last_torso_coords:
                    coord_text = f"Torso XYZ: {[f'{c:.2f}' for c in self.last_torso_coords]}"
                    cv2.putText(display_image, coord_text, (10, 60),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                
                cv2.imshow(window_name, display_image)
                
                # é”®ç›˜æ§åˆ¶
                key = cv2.waitKey(1) & 0xFF
                
                if key in (27, ord('q')):
                    break
                elif ord('0') <= key <= ord('9'):
                    self.current_target_index = key - ord('0')
                    print(f"\nğŸ¯ é€‰æ‹©ç›®æ ‡ç¼–å·: {self.current_target_index}")
                elif key == ord('n'):
                    try:
                        index = int(input("è¯·è¾“å…¥ç›®æ ‡ç¼–å· (0-35): "))
                        if 0 <= index <= 35:
                            self.current_target_index = index
                            print(f"ğŸ¯ é€‰æ‹©ç›®æ ‡ç¼–å·: {self.current_target_index}")
                        else:
                            print("âŒ ç¼–å·è¶…å‡ºèŒƒå›´")
                    except ValueError:
                        print("âŒ è¾“å…¥æ— æ•ˆ")
                elif key == ord(' '):
                    print(f"\nğŸ” å¼€å§‹æ£€æµ‹ç›®æ ‡åŒºåŸŸ {self.current_target_index}...")
                    result = self.detect_and_locate(
                        color_image, depth_raw, self.current_target_index
                    )
                    if result:
                        self.last_detection_result = result
                        self.last_torso_coords = result['torso_coord']
                        print("âœ… æ£€æµ‹æˆåŠŸ")
                    else:
                        self.last_detection_result = None
                        self.last_torso_coords = None
                elif key == ord('s'):
                    if self.last_detection_result:
                        self._save_result(color_image, self.last_detection_result)
                    else:
                        print("âŒ æ— å¯ä¿å­˜çš„ç»“æœ")
                # ğŸ†• æ–°å¢å¿«æ·é”®
                elif key == ord('+') or key == ord('='):
                    self.torso_z_tolerance += 0.01
                    self.depth_helper.torso_z_tolerance = self.torso_z_tolerance
                    print(f"ğŸ“ Zå®¹å·®: Â±{self.torso_z_tolerance*100:.0f}cm")
                elif key == ord('-') or key == ord('_'):
                    self.torso_z_tolerance = max(0.01, self.torso_z_tolerance - 0.01)
                    self.depth_helper.torso_z_tolerance = self.torso_z_tolerance
                    print(f"ğŸ“ Zå®¹å·®: Â±{self.torso_z_tolerance*100:.0f}cm")
                elif key == ord('['):
                    self.expected_torso_z -= 0.01
                    self.depth_helper.expected_torso_z = self.expected_torso_z
                    print(f"ğŸ“ ZåŸºå‡†: {self.expected_torso_z:.3f}m")
                elif key == ord(']'):
                    self.expected_torso_z += 0.01
                    self.depth_helper.expected_torso_z = self.expected_torso_z
                    print(f"ğŸ“ ZåŸºå‡†: {self.expected_torso_z:.3f}m")
        
        except KeyboardInterrupt:
            print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        finally:
            self.camera.stop()
            cv2.destroyAllWindows()
            print("[INFO] ç¨‹åºå·²é€€å‡º")
    
    def _save_result(self, color_image: np.ndarray, result: Dict[str, Any]):
        """ä¿å­˜æ£€æµ‹ç»“æœ"""
        output_dir = Path("data/screen_target_results")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        from datetime import datetime
        import json
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        vis_image = self.visualize_result(color_image, result)
        img_path = output_dir / f"target_{result['target_index']}_{timestamp}.png"
        cv2.imwrite(str(img_path), vis_image)
        
        json_path = output_dir / f"target_{result['target_index']}_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
        print(f"   å›¾åƒ: {img_path}")
        print(f"   æ•°æ®: {json_path}")


# ==========================================
# å‘½ä»¤è¡Œæ¥å£
# ==========================================
def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å±å¹•ç›®æ ‡å®šä½å™¨")
    parser.add_argument("--server", type=str, default="http://192.168.77.103:28000",
                       help="YOLOæœåŠ¡åœ°å€")
    parser.add_argument("--target", type=int, default=0,
                       help="ç›®æ ‡åŒºåŸŸç¼–å· (0-35)")
    parser.add_argument("--mode", type=str, default="interactive",
                       choices=["interactive", "single"],
                       help="è¿è¡Œæ¨¡å¼: interactive(äº¤äº’å¼) æˆ– single(å•æ¬¡æ£€æµ‹)")
    parser.add_argument("--torso-z", type=float, default=-0.17,
                       help="å±å¹•Torso ZåŸºå‡†å€¼ (ç±³)")
    parser.add_argument("--z-tolerance", type=float, default=0.05,
                       help="Zå€¼å®¹å·® (ç±³)")
    
    args = parser.parse_args()
    
    locator = ScreenTargetLocator(
        args.server,
        expected_torso_z=args.torso_z,
        torso_z_tolerance=args.z_tolerance
    )
    
    if args.mode == "interactive":
        locator.current_target_index = args.target
        locator.run_interactive()
    else:
        # å•æ¬¡æ£€æµ‹æ¨¡å¼
        if not locator.camera.start():
            print("âŒ æ‘„åƒå¤´å¯åŠ¨å¤±è´¥")
            return
        
        locator._init_depth_helper()
        
        try:
            print("â³ ç­‰å¾…ç¨³å®šå¸§...")
            import time
            time.sleep(2)
            
            color_image, depth_raw, _ = locator.camera.get_frames()
            
            if color_image is not None and depth_raw is not None:
                result = locator.detect_and_locate(color_image, depth_raw, args.target)
                
                if result:
                    print("\nâœ… æ£€æµ‹æˆåŠŸ:")
                    print(f"   ç›®æ ‡ç¼–å·: {result['target_index']}")
                    print(f"   åƒç´ åæ ‡: {result['pixel_coord']}")
                    print(f"   Torsoåæ ‡: {result['torso_coord']}")
                    print(f"   æ–¹æ³•: {result['method']}")
                    print(f"   Zåå·®: {result['torso_z_deviation']*100:.1f}cm")
                    
                    locator._save_result(color_image, result)
                else:
                    print("âŒ æ£€æµ‹å¤±è´¥")
            else:
                print("âŒ æ— æ³•è·å–å›¾åƒ")
                
        finally:
            locator.camera.stop()


if __name__ == "__main__":
    main()